# 4.1 基本流程

采用分治（divide-and-conquer）策略。

<div align="center"><img src="https://picgo-1305404921.cos.ap-shanghai.myqcloud.com/20210405133039.png" alt="image-20210405133031723" style="zoom:80%;" /></div>

可见决策树是一个递归过程，三种情况会结束递归生成叶子节点：（1）当前节点样本全部属于同一个属性、（2）当前属性集为空集或所有样本在所有属性上取值相同、（3）当前节点包含样本集合为空集。其中（2）利用当前节点的后验分布，（3）是把父节点的样本分布作为当前节点的先验分布。

# 4.2 划分选择

决策树关键在于选择最优划分属性，生成“纯度”（purity）尽可能高的样本。

## 4.2.1 信息增益

“信息熵”（information entropy）定义为：

$$Ent(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k$$

$Ent(D)$越小，纯度越高。

使用属性$a$进行划分后，减少的信息量称为“信息增益”（information gain）：

$$Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$$

信息增益越大，得到的“纯度提升”越大。ID3决策树学习算法就是以信息增益为准则选择划分属性。

## 4.2.2 增益率

不难发现，信息增益会偏向分类更多的属性，可能不利于模型的泛化能力。C4.5决策树算法使用了“增益率”（gain ratio）进行划分：

$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$

$$IV(a)=-\sum^V_{v=1}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}$$

$IV$称$a$的“固有值”（intrinsic value），实际上就是$D$在属性$a$的信息量。

增益率又可能会偏向分类较少的属性，C4.5使用了一种启发式方法：先从候选属性中挑选出信息增益高于平均值的，再选择增益率最大的。

## 4.2.3 基尼系数

CART（classification and regression tree）决策树使用“基尼系数”（Gini index）进行属性选择，基尼值公式如下：

$$Gini(D)=\sum^{|y|}_{k=1}\sum_{k'\ne k}p_kp_{k'}=\sum_{k=1}^{|y|}p_k(1-p_k)=1-\sum^{|y|}_{k=1}p_k^2$$

基尼值反映了随机抽选两个样本属性不同的概率，越小代表纯度越高。

计算属性$a$的基尼系数（$a$分成的所有类基尼值的加权平均）：

$$Gini\_index(D,a)=\sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)$$

# 4.3 剪枝处理

剪枝（pruning）可以减轻过拟合现象，分“预剪枝”（prepruning）和“后剪枝”（postpruning）两种：预剪枝是指在生成过程中，如果当前节点不能带来泛化性能的提升，则停止划分；后剪枝是指训练结束后，自下而上地搜索，如果将某一子树替换成叶子节点可以带来泛化性能的提升则进行更换。

## 4.3.1 预剪枝

