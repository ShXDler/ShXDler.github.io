*原文地址：ACL2019-*[*OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs*](https://www.aclweb.org/anthology/P19-1081/)*-Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba*

#   

# 1.简介

开放式对话系统的实现主要建立在提供与对话内容相关的实体和属性来保证对话内容的连贯性和一致性，本文通过提出一个基于注意力机制的图解码器，在内容量极大的知识图谱中搜寻最优路径，修剪候选实体，并通过零样本学习模型，利用前轮对话的已知信息对候选实体进行重新排序。同时，作者还构建了新的并行开放式对话系统$\leftrightarrow$知识图谱语料库（OpenDialKG），将对话中的实体与人工标注的知识图谱路径连接起来。 

![img](https://pic2.zhimg.com/v2-6e61d089a632cdd3057470ae38efe9d9_b.png)

图1 （a）开放式推荐对话系统 （b）知识图谱

# 2.研究方法

本文的核心思想是利用本轮对话及前轮对话内容，结合深度学习算法在知识图谱中进行路径的最优化，以提供最符合下轮与其谈话内容的结果。产生每轮对话算法的输入都为一个三元组 $\{x_e,x_s,x_d\}$ 组成， $x _e$ 为本轮对话的实体集合， $x_s$ 为实体周围的文本内容， $x_d $为前轮对话的内容。而输出则由二元组 $\{y_e,y_r\}$ 组成，分别代表实体路径和关系路径。目标则为找到使得 $score(f_{x\rightarrow y}(x),y')$ 得分函数最大的输出 $y$ ，这样就得到了下一轮对话所包含的实体及其最优路径。

![img](https://pic4.zhimg.com/v2-e6b8c2d1d3ff9f7065f30a7a9b98dfdb_b.png)

图2 模型算法架构

## 2.1 输入编码

**实体表示**：本文使用了知识图谱嵌入的方法对对话提到的实体进行编码，这种方法可以让意思相近的实体在嵌入空间里的分布也更接近，嵌入模型为：

$P\{\mathbb{I}_r(s,o)=1|\theta\}=score(\bf{e}(s),\bf{e}_r(r),\bf{e}(o))\\$ 

其中 $s,r,o$ 分别代表三元组中的主体、关系和客体， $score(\cdot)$ 函数充当了有效关系三元组的似然函数。

**句子表示**：对实体周围字词的表示使用了目前最好的模型之一——Bi-LSTM，结合GloVe分布单词嵌入，在Wikipedia和Gigaword语料库进行训练。

**对话表示**：本文使用了分层Bi-LSTM方法对前轮对话进行表示，同时设置了一个固定的窗口大小确定回顾对话的轮数。另外，模型还对前轮对话使用注意力机制，根据每轮对话与目标任务的关联性来增强或减弱其对模型的影响。

**输入综合**：在对实体、句子和对话分别确定了表示机制后，文章借鉴了模态注意力（modality attention）机制的方法，根据每个模态对任务的关联性，有选择地增强或减弱它们。

## 2.2 图解码器

通过运用以上方法提取的实体和周围内容的信息后，本文通过构建网络来进行相关实体的预测。基于知识图谱嵌入的思想，得到目标函数如下：

$\min_\bf{W}\cal{L}_f(\bf{x},\bf{y}_\rm{e};\bf{W}_f,W_p)+\cal{L}_{\rm{walk}}(\bf x,y\cal_p;\bf W_p)\\$ 

该目标函数使用了$\cal L_f(\cdot)$ 作为监督损失函数，而 $\cal L_\rm{walk}(\cdot)$ 则为知识图谱中衡量路径最优化的损失函数， $\bf W_f$ ， $\bf W_p$ ， $\bf W\rm _{input}$ 均为可学习的参数，分别作为最终得到的实体的分类器,路径搜寻模型和输入编码器。在优化过程中，本文也使用了权重递减的正则化项。

## 2.3 零样本相关分数

本文在知识图谱嵌入空间里使用了零样本相关分数（zeroshot relevance score），提升了模型预测的稳健性，同时也可以在未知领域获得更好的效果。具体来说，在选择损失函数时，本文挑选了可监督的hinge rank损失函数：

$\sum_i  \sum_{{\bf{\tilde{y}}}\neq {\bf y}_{e}^{(i)}}\max[0,{\bf\tilde y }\cdot {\bf y}_{e}^{( \it i )} -{\bf f}({\bf \bar{x}}^{(i)})\cdot({\bf y}_ e^{( i)}-{\bf\tilde y})^\top]\\$ 

其中 $\bf{f} \rm (\cdot)$ 是一个可以在知识图谱中游走并输出预测实体的变换函数，而 $\bf \tilde y$ 则是从知识图谱实体中随机抽样出的负例。直观上讲，投影嵌入值和标签正确的点积值会更大，而标签错误的则会更小。 $(\bf\tilde y \cdot y\it_e^{\rm(\it i\rm )}\rm)$ 则代表标记正例和负例的相关性。

## 2.4 知识图谱路径游走

如果产生的候选实体仅仅以相关性得分为依据，那么在指数级别的搜索空间里，模型的预测效果可能不尽如人意。因此，本文定义了基于注意力机制的DialKG图解码器模型，用于修建多余的路径减小搜索空间。解码步骤是典型的LSTM结构，首先基于如下所示的可游走路径的注意力机制，产生 $t$ 步解码时对话内容向量： $\alpha_t=\sigma({\bf W}_{h\alpha}{\bf h}_{t-1}+{\bf W}_{x\alpha}\bar{{\rm x}_t})\\ {\bf z}_t={\bf h}_{t-1}+\sum_{{\bf r}_k\in{\bf R}_{KG}}\alpha_{t,k}{\bf r}_k$ 

这里 ${\bf z}_t $ 即为经过其前轮对话实体得到的结果实体文本向量，进而得到：${\bf i}_t =\sigma({\bf W}_{hi}{\bf h}_{t-1}+{\bf W}_{ci}{\bf c}_{t-1})\\ {\bf c}_t = (1-{\bf i}_t)\odot{\bf c}_{t-1}+{\bf i}_t\odot\tanh({\bf W}_{zc}{\bf z}_t+{\bf W}_{hc}{\bf h}_{t-1})\\ {\bf o}_t = \sigma({\bf W}_{zo}{\bf z}_t +{\bf W}_{ho}{\bf h}_{t-1}+{\bf W}_{co}{\bf c}_t)\\ {\bf h}_t={\rm WALK}(\bar x ,{\bf z}_t)={\bf o}_t\odot\tanh({\bf c}_t)$

为训练上面得到的图解码器，本文结合了标注过的游走路径，计算了预测路径和输出结果之间的损失函数 ${\cal L}_{\text{walk}}({\bf x,y})=\sum_{i,t}{\cal L}_{\text{ent}}+{\cal L}_{\text{rel}}$ 。而模型训练完成之后，在每次解码的过程中，都可以对可能的路径计算得分并进行排序：

${\bf y}_{e,t}^{(i)}=\arg\max{\bf h}_t\cdot{\bf y}_e^{{(i)}^\top}+\sum\alpha_{t,k}{\bf r}_k\cdot{\bf y}_r^{{(i)}^\top}\\ $ 

在上式中，加号左边为零样本相关性得分，右边则为软性注意力输出路径得分。

**对抗迁移学习**：如果领域标签 ${\bf y}_d$ 是已知的，也可以是使用领域嵌入进一步的帮助特征抽取和路径最优化。作者也使用了对抗迁移学习的方法，训练出的模型在多领域的表现更佳。

# 3.数据集：OpenDialKG

作者在完成模型搭建的同时，也收集了一个新的数据集OpenDialKG。它涵盖了约91K轮次的对话，每段对话都结合了知识图谱的实体和联系，进行了路径的标注。该语料库及对应的知识图谱路径将人类对话中隐含的推理过程转化为知识图谱中的离散型操作。

**“绿野仙踪”构架**：在生成对话的过程中，第一个代理人使用的是种子实体，并基于该实体初始化对话，第二个代理人则在与第一个实体相关的一系列事实中，挑选最自然最相关的几个来进行回复，这些初始的路径往往都是单跳或双跳的。在第二个代理人完成回复后，知识图谱会结合最新一轮对话的信息，提供不同的多跳事实进行循环。这一过程可以让对话参与者同时对新的事实或实体以及相关知识图谱的路径进行标注。全过程交由参与者进行对话的终止，平均包括6次循环。而在这一过程中，作者进行了两个分离的收集过程：推荐部分和闲聊部分。推荐部分的题材设定为电影和书籍，闲聊部分则为运动和音乐主题。

# 4.模型评估

本模型的任务在于通过分析本轮和历史轮次对话的信息，通过知识图谱产生一系列可以形成与人类对话内容相似的实体。不同于端到端的句子生成，该模型更注重于在开放领域对话中的推理和知识检索任务。在评估过程中，作者使用了当下在外部知识增强对话系统中几个前沿模型作为基准线，包括seq2seq、Tri-LSTM等。而在模型内部，作者也对输入的三部分：实体、句子、对话文本进行前向选择，比较不同模型的效果。结果发现，在领域内、跨领域、人类评估三个方面，本文提出的全模型（包括实体+句子+对话）效果皆领先目前的先进方法。与其他模型相比，该模型能够产生更多的多跳结果，提升对话系统答案的多样性，并且解决了某些模型无法充分利用历史信息的缺点。值得一提的是，在迁移学习方面，本模型也能够在同时训练源数据和目标数据的基础之上，适应资源较少的新领域以此实现跨领域的学习。

# 5.结果论述

和之前的模型相比，该模型在不同方面都有独特的优点。在**知识增强对话系统**方面，该模型在结构化的知识图谱中给出了推理路径的显式表达，同时引入了注意力机制进行多跳概念的解码。在**端到端对话系统**方面，OpenDialKG语料库包含了多场景下的开放式对话，并用更灵活的知识图谱路径取代了传统的路径追踪方法。在**知识图谱嵌入推断**方面，该模型学习最优路径的方法是基于已有的路径而不是知识图谱的边缘预测。综合来看，该模型的创新点在于提出了基于注意力的图解码器，修剪较大的搜索空间，同时，零样本相关模型也使得相关性的量化排序成为可能，有效提升了语言对话的自然性。