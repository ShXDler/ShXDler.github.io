*原文地址：[A Survey of Knowledge-Enhanced Text Generation](https://arxiv.org/abs/2010.04389)-Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, Meng Jiang*

## 1.文章简介

在自然语言生成的过程中，利用外部知识增强的模型得到了广泛的应用，本文的内容主要建立在两方面：知识增强的普遍方法和架构，以及针对不同形式的数据的具体操作技术及应用。

文本生成任务同时指的是“文本到文本”的生成过程，即以文本作为输入，并将输入文本进行语义表示，最后生成期望输入的过程。机器翻译、摘要生成、问题回答和对话系统都是常见的文本生成任务。而文本生成的一个基本释义就是从给定输入序列生成期望序列的过程，即序列到序列（Seq2Seq）任务。Seq2Seq最先在2014年被提出，使用编码器-解码器的思想，编码器首先将输入序列编码成固定长度的向量，再由解码器生成输出向量。许多模型如RNN、CNN和Transformer等都被研究人员用来完成这一任务，而在诸多模型之中，基于**注意力**或**拷贝/指针**（copy/pointer）的方法得到了广泛的应用。

尽管如此，输入序列变量往往只带有很少的信息，因此在许多任务上，只依赖Seq2Seq任务的效果并不理想。比如对话系统中，机器会倾向于生成一些较为常见而**缺少具体含义**的语句，但人类的回复会更具备真正意义，这是因为人类会使用外部知识。比如**对话过程**中，人类可以从相关领域里选取符合对话内容的概念；在摘要生成中，人类可以从输入文档中选择关键词并选择合适的情态来保证语法上的正确性；而在**问答**的过程中，人类会使用与问题相关的常识知识或专业知识来推断答案。因此，知识在富有信息的语言生成中扮演着重要的作用。

### 1.1 文本生成过程

知识增强文本生成指的是从不同形式或来源中提取信息，用于不同的神经网络学习方法中，来提高文本生成的过程。在对话系统中，知识增强的Seq2Seq方法可以让模型更好地理解输入序列的语义含义以生成更有含义的回复。同样，在摘要生成、问题回答和故事生成的过程中，知识图谱的参与能够让机器生成更具细节的语言，提升文本生成方面的表现。

### 1.2 调查意义及文章架构

当前大多数关于知识增强的文献综述往往都较为片面而不够深入，而本文则为NLG研究者们提供了当下研究的综合内容及研究方向的指南，同时，作者们也对近年来NLG模型使用的GNN、强化学习和神经主题模型（neural topic model）进行了深入的讨论。

当下，NLG面临的主要挑战是如何从多样的信息来源中攫取有用的信息，第二大挑战则是使用合适的方法理解得到的知识并将其运用至文本生成当中去。针对第一个问题，本文在Section2和Section3-5两部分分别对知识增强的普遍方法和不同信息来源的具体方法进行讨论，而针对第二个问题，作者依据信息提取和利用的方法将近来较为火热的模型进行了分类。最后，本文也在Section6-7部分总结了近五年来NLG模型的发展，并预测了未来发展的七大主流趋势。

<div align="center">
    <img src="https://pic3.zhimg.com/v2-440d731b31a485f3fe0e3e0472412822_b.png">  
</div>

## 2.NLG中知识融合的基本方式

### 2.1 文本生成的基础模型

早期编码器-解码器框架建立在RNN的基础上，而近年来CNN和Transformer网络也得到了广泛的应用。根本上来说，编码器将变长序列编码为定长序列，再由解码器进行解码。从概率论角度来看，编码器-解码器是通过计算给定输入序列后计算变长序列的条件分布：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-4ce7f41e693f77923eb5037ecff5bd90_b.png">  
</div>

**RNN-Seq2Seq**

在**编码**阶段，模型首先按顺序对输入序列 $X$ 进行嵌入处理为 ${\bf e}(x_i)$ ，并使用基于RNN的编码器（如GRU，LSTM等） $f_{rnn-en}(\cdot)$ 进行编码，得到当前位置的隐藏层状态 ${\bf h}_i$ ： ${\bf h}_i=f_{rnn-en}({\bf e}(x_i),{\bf h}_{h-1})$ 。另外，整个输入序列的表示 ${\bf h}_n$ 记做 ${\bf c}$ 。

而在**解码**阶段，模型使用另一个基于RNN的函数 $f_{rnn-de}(\cdot)$ 进行解码，基于上一时刻输出序列单词的嵌入 ${\bf e}(y_{t-1})$ 和 $\bf c$ 决定当前时刻的输出序列隐藏层 ${\bf s}_t=f_{rnn-de}({\bf s}_{t-1},{\bf e}(y_{t-1}),{\bf c})$ ，同时结合非线性的多层感知机（MLP）函数，计算得到当前时刻下输出序列单词的条件概率 $p(y_t|y_{t-1},y_{t-2},\cdots,y_1,{\bf c})=f_{mlp}({\bf s}_t,{\bf e}(y_{t-1}),{\bf c})$ 。

**Transformer**

在**编码**阶段，模型将输入序列转化为连续的表示序列。这里的编码器 $f_{tf-en}(\cdot)$由重复的相同层构成，而每一层又包含两个亚层：第一亚层是多头自注意力网络，第二层则是位置全连接前馈网络，得到表示序列$({\bf h}_1,\cdots,{\bf h}_n)=f_{tf-en}({\bf e}(x_1),\cdots,{\bf e}(x_n))$。

而在**解码**阶段，解码器同样由多个相同的层组成，与编码器不同的是，解码器加入了第三个亚层，它能够对编码状态 ${\bf H}=({\bf h}_1,\cdots,{\bf h}_n)$ 进行多头自注意力计算，从而能够一次一个元素地生成输出序列。Transformer模型是自回归的，它在每一步将已经生成的符号作为额外输入生成下一符号，并利用缓存矩阵 ${\bf S}_t$提升生成效率，其中${\bf S}_t=f_{tf-de}({\bf S}_{t-1},{\bf e}(y_{t-1}),{\bf H})$ 是通过**键值对**方法计算得到的，最后一层的 ${\bf S}_t$ 可以被看作一个logit向量。

**优化方法**

文本生成的过程可以抽象地看成序列型多标签分类问题，可以直接通过负对数似然函数（NLL）进行优化，通过极大似然估计得到的目标函数如下： $\mathcal{L}_{NLL}(\theta)=-\log_{p_\theta}(Y|X)=-\sum^m_{t=1}\log(p_\theta(y_t|y_{<t},X))$ 

### 2.2 知识增强模型架构

当前最流行的知识增强文本生成方法是通过设计特定的模型架构来利用具体的知识，以下，作者列举了几个模型结合知识的普遍方法。

#### 2.2.1 注意力机制

获取编码器和解码器每一步的权重十分有用。在**解码**阶段计算隐状态 ${\bf s}_t$ 时加入了上下文向量 ${\bf c}_t$ ： ${\bf s}_t=f_{de}({\bf s}_{t-1},{\bf e}(y_{t-1}),{\bf c}_t)$ 。与原始Seq2Seq模型不同，这里的条件概率建立在目标单词 $y_t$ 对应的上下文向量 ${\bf c}_t$ 基础上，而 ${\bf c}_t$ 则依赖于输入序列编码得到的隐状态 $\{\bf{h}_i\}^n_{i=1}$ 的softmax加权和： ${\bf c}_t=\sum^n_{i=1}\alpha_{ti}{\bf h}_i,{\rm where}\ \alpha_{ti}=\frac{\exp(\eta({\bf s}_{t-1},{\bf h}_i))}{\Sigma^n_{k=1}\exp(\eta({\bf s}_{t-1},{\bf h}_k))}$ ，这里的 $\alpha_{ti}$ 反映了输入序列隐藏状态的重要性，而 $\eta(\cdot)$ 常用一个多层感知机函数，使得损失函数能够通过反向传播方法计算梯度，还有几种其他的方法如下表所示：

<div align="center">
    <img src="https://pic3.zhimg.com/v2-29e85a37421792943bd7ce1d503acfde_b.png">  
</div>

**知识相关注意力**

近年来的知识增强NLG工作广泛地使用了注意力的机制来进行知识表示和解码阶段的结合，这一总体思想是把隐藏层上下文向量 ${\bf c}_t$ 和知识上下文向量 ${\bf c}_t^K$ 结合起来更新解码器参数，来学习知识感知的上下文向量 $\tilde {\bf {c}_t}$ （如 $\tilde{\bf {c}_t}=f_{mlp}({\bf c}_t\oplus{\bf c}_t^K)$ ），下表总结了知识注意力的不同使用方法，包括话题、关键词、知识库和知识图谱等：

<div align="center">
    <img src="https://pic3.zhimg.com/v2-43bc309ef3e7e831b787b1e068934eb2_b.png">  
</div>

#### 2.2.2 拷贝指针机制

拷贝指针机制往往被用来从输入序列中选择子序列并将它们放至输出序列的合适位置上去。

**CopyNet**

CopyNet有着独特的网络架构，可以通过端到端的方式进行训练。在CopyNet中，生成目标单词的概率是两种模式概率的组合，**生成模式**和**拷贝模式**。首先，CopyNet对全局词汇表 $\mathcal V$ 和源序列词汇表 ${\mathcal V}_x$ 中每一个单词进行表示，构建了扩增词汇表 ${\mathcal V}_{ext}={\mathcal V}\cup{\mathcal V}_x\cup\{\rm unk\}$ ，得到扩增词汇表 ${\mathcal V}_{ext}$ 上的概率分布 $p(y_t)=p_g(y_t)+p_c(y_t)$ ，等式右边分别代表下式计算得到的**生成模式**和**拷贝模式**的概率：

<div align="center">
    <img src="https://pic4.zhimg.com/v2-7cd5057acef05cb97a16da8d49381edb_b.png">  
</div>

其中 $Z$ 为两种模式共享的标准化项， $\psi_g(\cdot)$ 和 $\psi_c(\cdot)$ 分别代表两种模式的得分函数：

<div align="center">
    <img src="https://pic2.zhimg.com/v2-1524cc7d64e50a0affb0450de6fcc1f1_b.png">  
</div>

其中 ${\bf v}_i$ 为 $v_i$ 的one-hot示性向量， $\psi({\cdot})$ 也存在其他的形式。

**指针生成器网络（Pointer-Generator Network, PGN）**

PGN也有独特的网络架构。不同的是，PGN补充了生成模式和拷贝模式之间的**切换**概率 $p_m$ ，得到的 ${\mathcal V}_{ext}$ 上的概率分布 $p(y_t)=p_m({\rm g})\cdot p_g(y_t)+(1-p_m({\rm g}))\cdot p_c(y_t)$ 。这里切换概率 $p_m$ 代表选择生成模式的概率：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-2e4c483e5cbad24612b9d859f883a6e0_b.png">  
</div>

而上式中注意力系数 $\alpha_{tj}$ 与前文计算方法相同，词汇表概率分布则计算如下：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-6861a4105ee8e551ab0f23a7949aca78_b.png">  
</div>

由此可见，CopyNet和PGN都可以看成两种模式之间的软切换，这两个模型都被许多NLG工作用作基础模型。

**知识相关模式**

知识相关的模式可以通过结合知识的方式，帮助NLG模型生成全局库和输入序列中没有的单词。此时，生成目标单词的概率成为三种模式概率的组合：**生成模式**、**拷贝模式**和**知识库模式**。下表总结了近年来几种不同的知识相关模式：

<div align="center">
    <img src="https://pic3.zhimg.com/v2-697e303e42e084b695ece3e9e5affac6_b.png">  
</div>

#### 2.2.3 记忆网络

记忆网络（MemNN）是建立在大型外部存储记忆上的循环注意力模型。记忆网络通常将外部记忆写入几个嵌入矩阵，并使用查询向量（普遍来讲是输入序列 $X$ ）重复读取记忆。这种方法可以编码较长的对话历史并记住外部信息。

给定记忆存储的输入集合 $\{m_1,\cdots,m_i\}$ 后，MemNN会将记忆存储表示成一系列可训练的嵌入矩阵 ${\bf C}=\{  {\bf C}^1,\cdots,{\bf C}^{K+1}\}$ 和一个查询向量 ${\bf h}_X^k$ 。模型会循环 $K$ 次并计算 $k$ 轮对每个记忆 $m_i$ 的注意力权重： ${\bf p}_i^k={\rm softmax}(({\bf h}^k_X)^\top{\bf C}^k(m_i))$ ，这里的 ${\bf p}^k$ 是一个软性的记忆选择器，决定了记忆和查询向量之间的相关性。

接着，模型会使用权重读出记忆 ${\bf o}^k=\sum_i{\bf p}^k_i{\bf C}^{k+1}_i$ ，作为解码步骤的输入，并更新查询向量得到 ${\bf h}^{k+1}_X={\bf h}^{k}_X+{\bf o}^k$ 。

另外，近来也有许多模型探索了将**外部知识和记忆进行结合**的方法，在从对话历史记忆中获取信息方面得到了不错的成果。

#### 2.2.4 图网络

图网络能够在传递信息的过程中捕获节点之间的依赖关系。近来，对图神经网络（GNN）和Graph2Seq的研究在**图表示学习**和**文本生成**之间建立起了联系。现代GNN往往使用的是近邻聚合方法，能够通过聚合近邻节点和边上的信息，不断迭代更新节点表示。经过 $k$ 次迭代后，节点表示信息就可以捕获到 $k$ 跳近邻的结构化信息：

<div align="center">
    <img src="https://pic3.zhimg.com/v2-a8cea5e07f77fa40483aa65b0e63fc12_b.png">  
</div>

这里 ${\mathcal N}(u)=\{(u_i,e_{ij},u_j)\in{\mathcal E}|u_i=u{\rm \ or\ }u_j=u\}$ 代表包含节点 $u$ 的边的集合， ${\bf u}^{(k)}$ 和 ${\bf e}_{ij}^{(k)}$ 代表对应节点和对应边在第 $k$ 次迭代的特征向量。 $\rm A{\scriptstyle GGREGATE}(\cdot)$ 函数通常被用在标注图中作为关系图建模的GNN，比如在GAT中：

<div align="center">
    <img src="https://pic2.zhimg.com/v2-4cd5f71e554bd250e7d6ddc2d4c8fe01_b.png">  
</div>

其中 $\sigma(\cdot)$ 是非线性的激活函数（通常使用LeakyReLU），而 ${\bf W}_i$ ，${\bf W}_j$，${\bf W}_k $ 则均为可训练的权重矩阵。图注意力机制也可以是多头注意力的，其中的注意力权重衡量了连接两个节点的边的联系程度。另外，研究人员经常使用 ${\bf h}_{\mathcal G}=\rm {R}\scriptstyle{EADOUT}(\{\bf {u}^{(K)},u\in{\mathcal U}\})$ 来得到图 $\mathcal G$ 的表示， $\rm {R}\scriptstyle{EADOUT}(\cdot)$ 常用平均或池化函数等简单的排列恒等函数。

在实际应用中，图网络经常被用于整合不同的知识图谱结构，如知识图谱和依赖图谱，而图注意力网络也能够非常容易地与序列注意力相结合，并进行联合优化。

#### 2.2.5 预训练模型

由于大多数NLG数据集的大小十分有限，而模型参数由相对而言较多，深度神经网络可能难以在特定任务上实现良好的泛化效果。然而无标注的数据集非常容易获得，加之计算机视觉领域的预训练模型大获成功，在NLG任务进行预训练可能让模型得到更好的初始化和正则化。第一代预训练模型如Word2Vec和GloVe，旨在学习非上下文嵌入。而第二代预训练模型如GPT和BERT则关注与学习上下文单词嵌入。

**非上下文嵌入**

这种方法在嵌入分布空间里将离散的语言符号表示出来，常用的模型如Word2Vec和GloVe可以得到单词的语义并对生成任务进行初始化。然而这种方法也存在两大局限：首先，嵌入是静态的，不能表示多义词和单词在上下文里的信息；第二，词汇表外的单词也无法进行嵌入。

**上下文嵌入**

为了解决多义词和依赖上下文的词义问题，研究人员提出了在大型语料库上使用上下文的预训练模型。主要思想是让 ${\bf x}_i$ 的上下文表示依赖全文 $({\bf h}_1,\cdots,{\bf h}_n)=f_{context}(x_1,\cdots,x_n)$ ，其中 $f_{context}(\cdot)$ 可以选择LSTM或者Transformer网络。比如ELMo使用的是双向LSTM网络，OpenAI's GPT使用了Transformer。而GPT的局限之一是只应用了单向的信息，而BERT利用遮盖语言模型（masked language model, MLM），从而克服了单向语言模型的问题。而BERT也延伸至许多其他使用MLM架构的方法如MASS和BART。在实际应用中，近期的许多研究也在探讨如何使用知识库中的三元组信息，将常识知识迁移至预训练模型中。

### 2.3 知识增强学习和推断

除了模型架构的选择，在学习和推断过程中使用知识信息也是常见的方法。

#### 2.3.1 知识相关任务学习

**知识作为目标**

知识相关任务与标准文本生成任务不同，它以知识为基础设立学习目标，辅助文本生成任务的完成，形成了多任务学习环境。例如知识损失（knowledge loss）定义为预测和真实知识语句之间的交叉熵，它可以和标准对话生成损失结合来增强对话生成。其他起相同作用的还有关键词提取损失（keyword extraction loss）、模板重排损失（template re-ranking loss）、模式损失（mode loss）和BOW loss等等。

另外，人们也可以从知识中获取文本生成的目标，并用它们监督标准文本生成任务，这种方法也被叫做弱监督学习。它强化了知识和目标序列间的相关性。例如在基于视角的摘要生成问题中，模型自动地基于外部知识库建立了目标摘要，它可以被用来监督摘要生成模型的训练。

**知识作为条件**

设计知识相关任务的另一常用方法是让文本生成以知识付附加条件，也就是说，目标为学习函数 $p_\theta(Y|X,K)$ 。普遍来讲，首先要从外部给定或者从外部资源中获得、亦或是从给定的输入文本中提取得到知识 $K$ ，再由条件文本生成模型结合知识生成目标输出序列。实际应用中，知识往往被诸如注意力机制或拷贝/指针机制等软强制算法进行纠正。把知识看做条件的思想在知识增强文本生成中被广泛的应用，比如在人物对话回复中考虑角色、情感对语句的影响等等。

#### 2.3.2 知识约束学习

除了封装知识来设立单独学习任务之外，知识增强学习的第二种范式是将知识作为限制条件约束原有的文本生成训练目标。

后验正则化（posterior regularization, PR）框架被用来约束模型未标注数据后验空间，广泛用于概率模型上添加知识约束条件。PR在训练目标函数 ${\mathcal L}(\theta)$ 上添加了一个约束项来对相关知识进行编码，记约束函数为 $g(X,Y)\in{\mathbb R}$ ，函数值越大代表特定知识下生成的 $Y$ 越好。为了添加约束条件，PR引入了辅助分布 $q(Y|X)$ ，并在 $q$ 上通过添加促进 $g$ 的期望值更大的约束： $\mathbb{E}q[g(X,Y)]$ 。同时也使用了KL散度促进模型 $p_\theta$ 和 $q$ 接近。由此，该优化问题约束条件可以表示如下：
$$
\max_{\theta,q}{\mathcal L}(\theta)-{\rm KL}(q(Y|X)||p_{\theta}(Y|X))+\xi\\s.t.{\mathbb E}q[g(X,Y)]>\xi\\
$$
其中 $\xi$ 是一个松弛变量。PR框架也往往和其他的约束驱动学习方法进行结合。

#### 2.3.3 知识约束推断

预训练模型能够只用一个简单的对数似然目标函数利用起海量的未标注数据，然而这种模型一旦训练完成后，在不改变模型架构或微调的情况下再加入知识来控制文本生成就显得尤为困难了。即插即用语言模型（plug and play language model, PPLM）开辟了知识增强推断的新方法。在推断的每一步生成中，PPLM沿着两个梯度方向移动历史矩阵：一个是条件属性模型 $p(a|Y)$ 关于 $a$ 的对数似然函数梯度正方向，另一个是未修改的预训练生成模型 $p(Y|X)$ 对数似然函数梯度正方向（如GPT）。具体来讲，属性模型 $\Delta{\bf S}_t$ 基于 $p(a|Y)$ 的更新过程如下：

<div align="center">
    <img src="https://pic2.zhimg.com/v2-540a4422e57fc34bd762d3a67f15c5ed_b.png">  
</div>

其中 $\gamma$ 为标准化项的比例系数， $\Delta{\bf S}_t$ 为Transformer中历史矩阵 ${\bf S}_t$ 的更新值。经过多次迭代后，生成更新值 $\tilde{\bf S}_{t+1}=f_{de}(({\bf S}_t+\Delta{\bf S}_t),{\bf e}(y_t),{\bf H})$ ，进一步生成新的logit向量。PPLM在可微属性模型组合中展现出了良好的效果和灵活性。

## 3.主题、关键词和语义特征增强NLG

### 3.1 主题增强NLG

主题作为文本的表示和压缩形式，往往被用来保持语义上的连贯性。例如摘要生成需要从长输入文本中提取出较短的输出来捕获主题，对话系统则需要避免生成琐碎的话语，主题建模可以帮助发现文本的高层次内容。一个经典的主题模型隐含狄利克雷分布（Latent Dirichlet allocation，LDA）被用来捕捉单词和文本中的隐含语义关系，从而产生一个低维表示。在LDA中，每个主题都被定义为单词上的分布，每个文档被定义为主题上的混合分布，后面的生成便是基于这两个分布的。近期的一些研究开创了新的方法，帮助模型学习低维表示。

常见的需要考虑主题的NLG任务包括对话系统、文本总结和机器翻译等等。在对话系统领域，基础Seq2Seq模型往往只会产生琐碎而模棱两可的回复。而在文本总结方面，在没有主题引导的情况下，由于输入序列远远长于输出序列，所以注意力机制的表现也并不好。而对于机器翻译，纵使输入输出序列的语言不同，其内容和主题也是相似的，因此主题也可以在语言的转换期间起到保留语义信息的辅助作用。

下表总结了一些有代表性的主题增强NLG模型，它们可以被分为三类：利用模型里的主题词、联合优化模型和CNN主题模型、变分推断主题网络。

<div align="center">
    <img src="https://pic4.zhimg.com/v2-47ed7a0abf590e577c4290991024f13b_b.png">  
</div>

#### 3.1.1 利用生成主题模型里的主题词

主题可以帮助理解语义信息，同时也唯一地决定了语义谱。比如人类的对话过程中，人们会首先在脑海中选择一个和主题相关的概念，再选择词语组织内容进行表达。而利用生成主题模型的方法首先使用生成主题模型（如LDA）发现主题，然后使用主题注意力机制将主题表示融入进神经生成模型里。在现有工作中，表示主题主要有两种主流方法。一是使用每个单词的生成主题分布；二是对每一个输入序列分配一个具体的主题，选择其中k个概率最高的。使用明确的主题词可能比主题分布有着更强的引导性，然而这种引导性在生成主题词不相关的情况下，也可能让目标输出序列偏离。

在近期研究中，Zhang等人提出使用编码器解码器隐状态合并主题分布的Seq2Seq模型 。而Xing等人设计了Topic-Seq2Seq模型来将主题词作为先验知识 。具体来讲，主题词首先从预训练LDA模型中获得，用 $\{\bf{p}_i\}_{i=1}^k$ 进行表示。而在解码阶段，模型通过输入序列和主题词的联合注意力机制生成每个单词。解码器首先使用输入序列注意力 ${\bf c}_t$ 和主题注意力 ${\bf c}_t^{tp}$ ，更新隐状态 ${\bf s_t}=f_{de}({\bf s}_{t-1},{\bf c}_t,{\bf c}_t^{tp},{\bf e}(y_{t-1}))$ ，再通过生成模式和主题模式计算目标单词的概率。

Topic-Seq2Seq模型使用了LDA算法来提取主题，并被多项研究进行了拓展。Liu等人添加了两个惩罚项来监督主题分配的重要性和话题词的选择，这种组合两种正则化的方式要比普通的监督更为有效 。同时，Wang等人提出的主题感知ConvS2S模型能够表示长输入序列并发现远距离依赖，该模型能够避免RNN-Seq2Seq模型产生的梯度消失问题 。另外，他们还使用了强化学习的方法，从而可以在矩阵不可微的情况下直接优化模型，避免推断中产生的误差。Narayan等人则通过合并单词嵌入和主题分布向量作为编码器输入，从而增强解码器的表现 。

#### 3.1.2 联合优化生成模型和CNN主题模型

LDA模型通常对话题的单词分布进行狄利克雷分布的先验假设，然而LDA模型可能无法找到目标任务的合适话题，也不能很好地适应输入输出序列之间多样的依赖关系。而该方法设计的端到端神经网络可以在学习隐含主题表示的同时生成输出序列，而CNN常被用来通过迭代卷积池化操作来生成隐含主题。CNN在将隐含主题表示成主题向量方面引起了研究人员极大的兴趣，经验分析表明，基于卷积的主题提取在多种应用上的表现都超过了LDA模型，但是相关理论研究还不够充分，CNN模型的可解释性也不如LDA模型好。

#### 3.1.3 变分推断神经主题网络增强NLG

神经主题网络往往会比传统模型有着更多的参数，所以过拟合现象也会更为严重，变分推断则可以通过假设先验分布减轻了这一问题。同时，主题网络的特性让我们可以使用反向传播方法进行联合优化，寻找更连贯的主题。

神经主题网络结合了神经网络和概率主题模型的优势，能够通过反向传播方法高效地进行优化，并能很好地适应上下文信息。在主题模型中，文档在主题上存在多项式分布，而主题在单词上又存在另一个多项式分布，Cao等人将这一思想和使用可微函数嵌入思想的方法进行了结合 。为了提高推断的效果，模型往往使用狄利克雷分布作为文档的先验分布，以生成每个多项式分布的参数 $\theta_d$ 。LDA模型如下： $\theta_d\sim{\rm Dirichlet}(\alpha)$ ， $t_i\sim {\rm Multinomial}(\theta_d)$ ， $w_i\sim{\rm Multinomial}(\beta_{t_i})$ ，其中 $d$ 代表文档的表示， $t_i$ 表示每个单词 $w_i$ 分配的主题，而 $\beta_{t_i}$ 则代表每个给定主题 $t_i$ 后单词上主题的条件分布参数。文档 $d$ 的边际似然函数如下：

<div align="center">
    <img src="https://pic4.zhimg.com/v2-62d8363cde382e4dac1b6616db2d2ba7_b.png">  
</div>

然而，有向生成模型遇到建立低方差梯度估计的问题。Miao等人通过神经网络参数化多项分布的方式，使用变分推断进行了模型参数的联合学习 。他们创建了神经网络结构，用于构建基于多元正态分布的主题分布，记做 $\theta_d\sim {\rm G}(\mu_0,\sigma^2_0)$ ，其中 ${\rm G}(\mu_0,\sigma_0^2)$ 由基于各向同性的高斯分布 $N(\mu_0,\sigma_0^2)$ 的神经网络组成。由此， $d$ 的条件分布可改写成：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-751c1391635cf1c8adb864ba95c3c94c_b.png">  
</div>

与原式相比，隐变量 $\theta$ 是基于高斯分布的神经网络决定的参数。为了进行神经变分推断，Miao等人构建了推断网络 $q(\theta|\mu(d),\sigma(d))$ 来估计后验概率 $p(\theta|d)$ ，该神经网络由MLP完成 。使用正态分布作为先验分布可以重新计算参数，以构建无偏和低方差梯度的估计量。如果不使用共轭先验，参数的更新将会直接从变分下界中得到。文档对数似然的变分下界（ELBO）构造如下：

<div align="center">
    <img src="https://pic2.zhimg.com/v2-a446476884abc77d8d2cbb22ca561d39_b.png">  
</div>

其中 $q(\theta|d)$ 是估计真实先验 $p(\theta|d)$ 的变分分布，下界是通过从 $q(\theta|d)={\rm G}(\theta|\mu(d),\sigma^2(d))$ 对 $\theta$ 进行抽样得到的。

为了将主题模型和生成模型进行组合，常用的思想是变分自编码器（Variational Auto-Encoder，VAE），它使用自回归网络（如LSTM）作为编码器和解码器。VAE可以通过使用解码器重新构建文本学习文本隐含代码 $z$ ，它假设生成过程是被控制在连续的隐含空间里，这种VAE使用方法认为文本序列信息可以表示文本的语义结构。Wang等人提出提出了主题引导的变分自解码器（TGVAE），通过主题依赖高斯混合先验分布将主题知识并入隐变量中 。主题依赖高斯混合模型（GMM）定义如下：

<div align="center">
    <img src="https://pic4.zhimg.com/v2-4b638b1941ffe18ef0c40cbcc667f55f_b.png">  
</div>

$T$ 代表主题数量， $\mu(d)$ 和 $\sigma^2(d)$ 是MLP得到的函数。TGVAE过程使用了词袋作为输入，并将输入文档嵌入为话题向量，再使用话题向量重新构建词袋输入，最后学习单词上的主题分布来构建主题依赖先验模型，以生成输出序列。具体来讲，联合边际函数如下：

<div align="center">
    <img src="https://pic4.zhimg.com/v2-7e83abbd188d2a191b50fef48d32f84b_b.png">  
</div>

为了最大化对数似然函数 $\log p(Y,d|X)$ ，作者构建了变分目标函数：

<div align="center">
    <img src="https://pic4.zhimg.com/v2-607965c3ae67d5530826aeef628b7987_b.png">  
</div>

其中 $q(z|X)$ 是 $z$ 的变分分布。最后再组合两个目标函数 ${\mathcal J}={\mathcal J}_{topic}+{\mathcal J}_{seq2seq}$ 。

后续研究中，Fu等人使用变分分层主题感知机制（VHTM）对TGVAE进行了扩展，他们将主题知识融入进单词嵌入和段落注意力中，以实现长文本的摘要生成 。Li等人阐述了上述两种方法没有明确地将语义和结构隐含代码分离开 。他们使用了两个分离的自解码器，包含一个主题解码器和一个句子特征解码器，其中主题解码器也作为分类器驱使生成的文本和原文本的语义尽可能相同。通过分离这两个过程，模型可以独立地控制结构和语义信息，CM（content manipulation）等方法就应用了这一思想。

#### 3.1.4 主题增强NLG不同方法的优缺点

主题模型（如LDA）中，因为单词和文档的语义表示被组合到统一的框架中，所以它有着严格的概率表述。另外，主题模型很容易使用和整合到生成框架中。比如单词嵌入、主题注意力等等。然而，LDA模型由于和神经生成模型的训练过程是分离的，所以无法适应输入与输出序列间多样的依赖关系。

神经主题模型融合了神经网络和概率主题模型的优势，能够通过反向传播方法进行优化，也可以扩展到大数据集里。普遍来讲，神经主题模型的主题连贯性要比LDA更好，然而神经变分推断方法有着相同的劣势：它们都假设主题分布服从各向同性的高斯分布，因此无法计算主题之间的相关性。现存的神经主题模型在使用VAE时都要求文档独立同分布，然而实际上，文档之间都或多或少地存在相关性。

### 3.2 关键词增强NLG

关键词指的是那些概括表示了一篇文章内容的词语，主流的获取关键词的方法主要有以下两类：关键词分配和关键词提取。**关键词分配**指的是从预定义的术语库中选择关键词，关键词提取则是选择文章中最有代表性的词。**关键词提取**技术（如TF-IDF，TextRank，PMI）被许多NLG任务利用，来增强语义的连贯性。

对话系统和总结概括是采用关键词知识的两大主流任务。在**对话系统**中，关键词可以突出重点，帮助模型生成更有信息量的词。近期的许多工作也在研究关键词在个性对话中的作用，来更好的表达情感、人物性格和语气等等。而在**总结概括**中，模型往往很难控制并且会遗漏关键信息，使用关键词则可以给模型提供重要的线索，这一过程和人类的行为也十分相似。关键词增强的NLG方法主要可以分为以下两类：利用关键词分配和利用关键词提取的方法。

#### 3.2.1 利用关键词分配的方法

在关键词分配过程中，关键词都是提前储存在词库中，然后利用关键词分类器来基于输入序列进行分类。提前储存的好处之一是可以保证关键词的质量，不会出现词义相近或重复的关键词；另一好处是即使没有共同词的两篇文章词义相近，他们仍然会被分配到相同的关键词。然而，也有一些NLG场景并不适用预定义的词库。使用关键词分配的一个场景就是通过使用预定义的领域词，保证输入和输出序列的相关性；另一个场景则是结合特定的属性（如感情、性格）进行对话声称。

**给解码器分配关键词**：

分配关键词最直接的方法就是从预定义词库中选择关键词。然而，有的输入序列并没有明确的关键词，但我们可以从词库中找到一个。Li等人更详细地描述了这一问题，他们将编码器得到的隐状态的和输入进一个NLP中，以得到情感分类，并以此指导文本生成。 具体来讲，预测情感分类 $k\in{\mathcal K}$ 首先由一个实值低维向量 $\bf k$ 进行表示，再和上下文向量 ${\bf c}_t$ 进行结合，得到新的向量来生成文本。为了对情感的表达多少进行动态跟踪，Zhou等人在解码过程中使用了一个记忆模块 ：在解码过程开始之前，每个分类先用一个情感状态向量进行初始化，在每一步中，情感状态会以一定速率衰减，解码完成后将会衰减到0，代表情感已经表达完全。Xu等人则通过学习结合情感的回复生成和查询生成的双重任务，对二者的相互关系进行了建模。 

**给序列生成分配关键词**：

有研究提到，如果明确地融入情感关键词，生成语句的情感可能会被过度表达。Song等人提出了另一种增强情感强度的方法 ，不使用明确的情感词，而是把中立词语通过不同方式进行组合，来隐式地表达情感。具体来讲，他们构建了一个句子级别的分类器，判断语句是否在不使用过多明确情感词汇的情况下表达了某一情感，这一分类器被连接在解码器的最后。总体的生成损失函数如下：

<div align="center">
    <img src="https://pic4.zhimg.com/v2-0702b86b58c375e3a3e75d2d114a5f97_b.png">  
</div>

#### 3.2.2 利用关键词提取的方法

关键词提取是通过从文章中挑选最重要的单词进行的，提取的方法有很多，从统计方法（如TF-IDF，TextRank，PMI）到监督学习方法（如BiLSTM）等等。融合提取出的关键词和上一部分的方法相似，都是将关键词作为额外的输入加到解码器中。近期许多其他的研究在编码阶段加入了额外的序列编码器（如RNN）来表示关键词，记做 $\{\bf {k}_i\}^p_{i=1}$ ，然后解码器再结合单词嵌入和上下文向量进行隐状态更新 ${\bf s}_t=f_{de}({\bf s}_{t-1},{\bf e}(y_{t-1}),{\bf c}_t,{\bf c}_t^k))$ 。Li等人使用了一个双向LSTM来编码提取的关键词，并将正向和反向的最后一个隐藏状态 $\vec {\bf {k}_p}$ 和 $ \mathop{\bf {k}_1}^{\leftarrow}$ 连接作为关键上下文向量 。另外，Li等人还提出使用多任务学习来训练关键词提取网络和概括生成 ，这两个任务都是从输入文本中提取重要信息，所以它们可以共享参数，提升优化能力。而在实践中，他们将输入文本和总结概括重叠的词语作为关键词，采用BiLSTM-Softmax作为关键词提取器，总体损失函数如下：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-632e95c5a467778d9d7516bdf6dfd280_b.png">  
</div>

#### 3.2.3 不同方法的优缺点

关键词分配的主要优点就是可以保证关键词的质量，也能将两篇单词不同但意思相同的文章分配相同的关键词。它也存在两大缺点：一方面，在新的领域构建和维护词库的代价非常昂贵；另一方面，文章里有但词库里没有的关键词会被直接忽略。因此，关键词分配适合那些需要特定关键词指导信息生成的领域，如对话生成。

而关键词提取因为不依赖词库，所以使用起来非常便捷。它的缺点有二：一是语义相似的文本可能会被提取出不同的关键词，无法保证关键词的一致性；二是当输入文本难以找到合适的代表词语时，提取器往往会生成一个毫不相关的词语，误导后面文本的生成。因此，关键词提取适合那些输出序列需要保留输入序列中重要信息的任务，如总结概括和改写。

### 3.3 语义特征增强NLG

特征增强的编码器是指编码器不仅读入输入序列，还加入了辅助的人工特征。语言学特征是最常见的人工特征，包括词根（lemma）、词性（part-of-speech，POS）标注、命名实体标注（NER）、依赖性分析和语义分析等等。

#### 3.3.1 词形还原

在形态学和词典学中，词根是一系列拥有相同意思单词的字典形式和标准形式。在机器翻译过程中，对形态丰富的语言进行建模是一件很困难的事情，词形还原可以减少数据稀疏性，并让不同形式的同义词共享同一表示。

#### 3.3.2 词性（POS）标注和命名实体标注（NER）

POS标注可以对单词进行语法类别和词性的标注，NER标注可以将命名实体分成预定义的及各类别。使用POS标注和NER标注可以探测命名实体，并帮助模型更好地理解输入语句。

## 4.知识库和知识图谱、依赖图谱增强NLG

### 4.1 知识库增强NLG

NLG的一大挑战就是要从输入输出序列中发现元素间的相关关系，而这种关系实际上是不同形式的**知识**。知识库（knowledge base, KB）是一种收集、储存和管理大规模信息的技术，它包含了无数的由主谓宾构成的三元组（也称“事实 facts”）。近期许多研究者都开始探讨使用KB的方法，常用的KB包括DBpedia，Freebase和Wikidata。使用KB的一个流行任务是基于文本生成的问题回答，不同的问题可能有不同形式的答案，如果只依赖问题，生成的答案可能缺少常识知识。

在实际应用中，一个合适的答案应当以自然的、能被理解的语言进行表述，而不是只包含一个简单的实体。同时，完成对话系统、内容操纵（content manipulation，CM）也可以有KB的参与。需要注意的是基于KB生成的对话和问题回答也有不同之处：第一，对话往往是开放性的；第二，对话的过程需要回顾前几轮的内容。KB在识别依赖关系中扮演着重要的作用。下表总结了几种处理KB和输入输出序列关系的方法：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-14535027951ddcd1e1302500f174ef90_b.png">  
</div>

这些方法可以被分为三类：**基于KB的监督学习**、**基于KB的非监督学习**和**选择KB增强合并**。

#### 4.1.1 围绕KB设计监督任务来进行联合优化

知识库能够增强问题回答和对话系统。然而要设计有效地设计融合方法却较为困难，因为基础的Seq2Seq模型难以表示离散的、没有联系的概念。解决这一问题的思想就是进行多任务学习联合训练神经网络。例如目标任务是回答序列的生成，额外任务包括问题理解和事实检索，这样知识就能在一个统一的框架中得到共享。而问题理解和时间检索实际上是非常相关而有用的人物，因为一个问题的主语和谓语可以和KB中的三元组相匹配，这两个任务可以排除掉无关信息，减少干扰。

GenQA是第一项使用事实知识库进行问题回答的研究。在生成过程中，GenQA从KB中检索单词，但是它不能使用问题的相近词，也无法处理包含多个事实关系的问题。He等人发现并提出CoreQA模型改进了这一问题 。CoreQA使用了端到端的方式，同时采用复制和检索的机制进行答案生成。具体来说，该模型使用了一个检索模块来理解问题并搜索相关事实。检索模块通过匹配函数，基于MLP机制计算输入表示 ${\bf h}_X$ 和相关事实 $\{\bf {k}_i\}^N_{i=1}$ 的得分，得分函数也可以用双线性模型、规则模型等代替，而KB中的事实表示 $\bf k$ 则由主谓宾三个向量连接得到 ${\bf k}=({\bf k}^{(s)}\oplus{\bf k}^{(p)}\oplus{\bf k}^{(o)})$ ，再交由两个编码器对问题和事实进行表示。在解码阶段，解码器利用上述得到的向量进行隐状态更新： ${\bf s}_t=f_{de}({\bf s}_{t-1},{\bf c}_t,{\bf c}_t^{kb},{\bf e}(y_{t-1}))$ 。CoreQA通过计算三种模式的混合概率模型：①从全局词汇库中生成单词 ②Kb中检索事实单词 ③从输入序列中复制单词，来进行单词的预测。

CoreQA提供了一个普遍的端到端框架来利用KB中的信息生成答案，也被许多其他的研究进行了扩展。如Fu等人添加了异构存储器获取半结构实体的信息 ；Gao等人使用了Wasserstein距离建立对抗学习网络，提升答案的连贯性 。而对于对话系统，Madotto等人论述了CoreQA在多轮回答后利用历史KB信息上的不足，他们提出了Mem2Seq模型，结合多跳注意力机制和指针网络，使用记忆网络动态地更新信息 

#### 4.1.2 将KB作为序列条件因子的非监督方法

尽管输入和输出序列在语言或形式风格上有可能差别很大，但其内部却有着共同的信息。而KB保存事实事件的特性可以作为生成输入输出序列的条件。例如机器翻译和语言风格转变中就常用到对抗网络的方法。

自然语言有着许多不同的形式，近期也有研究对文本风格的转换感兴趣。一系列相关研究通过改变句子的性质实现这点，包括主动句和被动句的转化等等。然而，它们并没有对重要的内容进行更新，以维持性质改变前后的连贯性。所以，人们提出了内容操作（content manipulation，CM）任务，意思是根据输入语句的风格表述某一给定事实。这一任务的难点在于要非监督性地理解句子结构、参考原句改变内容以及改正语法提高流利度，这一过程需要需要模型在没有标准文本的条件下完成。

Wang等人提出了一个双目标优化模型，两个目标分别衡量内容的保真和风格的保留 。首先，内容保真度 $p(X'|k',X)$ 意味着使用风格 $X$ 和新事实 $k'$ ，计算新语句和人工写的 $X'$ 相同的概率。第二，风格保留度 $p(X|k,X)$ 意味着原有事实 $k'$ 的基础上重新构建的语句和原有语句 $X$ 相同的概率（如果模型正确输出序列应当等于其本身）。对两个目标进行组合，可以得到损失函数：

$${\mathcal L}_{CM}(\theta)={\mathcal L}_{content}(\theta)+{\mathcal L}_{style}(\theta)=\log p_\theta(X'|k',X)+\log p_\theta(X|k,X)$$ 

在该模型中，事实和参照风格的建模是分开的，Feng等人提出使用分层编码器，结合互动注意力机制进行二者融合的表示 ，从而可以高效地捕捉语义联系。同时模型还采用了回译的方法，构建伪训练对来增强内容保真度和风格保留度。

####  4.1.3 通过选择KB或事实增强合并

KB和序列间的相关性对句子生成中的知识发现十分重要，因此通过对KB和事实进行选择可以增强KB合并的表现。

KB结合的普遍方法是通过分析输入序列、检索相关信息来生成知识感知的输出。然而现实中，输入和输出序列的依赖关系和事实的相关性往往不能令人满意。Lian等人阐述了这一问题，原因在于对相同的输入而言，模型可能会选择和使用不同的知识事实来生成不同回复。对于特定的输入和输出对，二者对知识库的后验分布可以为知识选择提供额外的指导。而挑战就在于先验后验分布可能存在矛盾，具体来讲，如果模型只从先验分布中选择词语，它就很难在推断过程中得到正确的后验分布。

Lian 和Wu 等人通过使用先验分布估计后验分布的方法，进而在不直接使用后验分布的情况下选择合适的知识，解决了这一问题。他们引入了辅助损失函数KL散度来衡量二者分布的相似程度：

<div align="center">
    <img src="https://pic1.zhimg.com/v2-68b4f326fb4bff3283485050fb7fb96c_b.png">  
</div>

在最小化KL散度损失函数时，后验分布 $p(k|X,Y)$ 可以被看做标签，来应用先验分布 $p(k|X)$ 估计 $p(k|X,Y)$ ，最终的损失函数由KL散度和生成损失函数加和构成。

### 4.2 知识图谱增强NLG

作为结构化人类知识的知识图谱（knowledge graph, KG），在学术界和工业界都引发了研究人员极大的兴趣。KG是包含实体、关系和语义描述等事实的结构化表示，人们可以通过根据连接追溯实体如何与知识相互连接。知识库和知识图谱这两个概念经常被混用，但实际上知识图谱作为一种图，其中的实体间的连接是作为一级组成部分的。

在**应用**当中，**对话系统**是最流行使用KG的任务。一段对话的关注点会在概念之间跳转，并将对话分成几个部分，因此可以在KG中表示为连接多个实体的路径。同样，问题回答、创造性写作和机器翻译的大量工作也用到了KG。其中**科学写作**旨在一步步解释自然过程和自然现象，所以每一步也可以反映在KG的连接中，形成一段路径。在**故事生成**中，使用知识图谱中的知识可以促进对故事线的理解，更好的预测故事中会发生的事。而在**摘要总结**中，使用KG中的常识性知识生成的总结不会文中的事实冲突。KG中的表示生成了结构化的总结，并突出了相关概念之间的联系。

与使用独立、分离的三元组不同，使用KG能够让文本生成更加的富有语义。而在使用KG时，节点嵌入和连接路径扮演了重要的角色，相关技术有知识图谱嵌入（KGE）和基于路径的知识图谱推理等。另外，使用新兴的GNN和Graph2Seq encoder-decoder等方法可以让人们编码KG中的多跳和高阶关系。

值得一提的是，在本体KG中，每一条边都是基于一个固定的实体和关系库，不存在歧义。而在开放KG中，节点和边分别对应提及的实体和开放的关系，这一部分作者将先讨论本体KG。另外，基于KB的技术也可以通过将三元组表示为KG中的实体和边，从而应用于KG当中。

近来，研究人员已经开发了许多应用知识图谱的方法（如下表所示），这些方法可以被归为一下三大类：**知识图谱嵌入**、**路径搜寻**和**图神经网络**。

<div align="center">
    <img src="https://pic3.zhimg.com/v2-f56be996396f4a5bfa2732fca2d91072_b.png">  
</div>

#### 4.2.1 结合**知识图谱嵌入（KGE）**的语言生成

KGE技术能从KG中学习并将实体和关系嵌入进连续向量空间中，但是KGE只能依赖单跳关系路径，受到得分函数的严格限制。因为KGE能够提供实体节点间的连接性信息，所以能够捕捉到实体节点间的语义联系。这一主要思想是将实体和联系表示在低维空间 ${\mathbb R} ^d, d\ll|{\mathcal U}\cup{\mathcal R}|$ 中，从而在保留KG固有结构的同时对数据进行降维。这其中，TransE由其简易和有效的特点成为最常用的技术之一。在给定KG边 $(u_i,r,u_j)$ 后，TransE会将关系看做转义向量 $\bf r$ ，从而嵌入实体 ${\bf u}_i$ 和 ${\bf u}_j$ 可以以较低错误进行连接： ${\bf u}_i+{\bf r}\approx{\bf u}_j$ ，而构建负例的常用方法是替换KG边中的头或尾实体。接着，低维实体和关系的嵌入会通过SGD和L2正则化方法进行优化：

<div align="center">
    <img src="https://pic2.zhimg.com/v2-c9514d75e750eb6f91d0d51ed306d6ed_b.png">  
</div>

其中 $(u_i,r,u_j)$ 为正例， $(\bar{u}_i,\bar r,\bar u_j)$ 为负例， $\gamma$ 代表边际。由此，NLG过程就可以在解码和编码阶段使用嵌入的实体和关系表示了。

**编码增强**：在编码过程中，实体节点先通过连接函数 $u=\psi(v,X)$ 进行表示，然后再将单词表示 $\bf x$ 和相应的实体表示 $\bf u$ 两个通道进行结合（如 $\bf u \oplus x$ ）来增强编码器。

**解码增强**：在解码过程中，解码器会在隐藏层表示和序列相关子图 ${\mathcal G}_{\rm sub}$ 中的边集 ${\mathcal E}_{\rm sub}$ 中使用注意力机制来更新状态 ${\bf s}_t$ ： ${\bf s}_t=f_{de}({\bf s}_{t-1},{\bf c}_t,{\bf c}_{t-1}^{ke},{\bf e}(y_{t-1}))$ ，其中 ${\bf c}_t$ 是注意力机制中的隐藏层加权和， ${\bf c}_t^{ke}$ 计算每一条边的注意力。

#### 4.2.2 通过**路径寻找策略（PF）**进行推断

KGE使用了单跳关系路径并通过语义联系学习了节点的表示方法，然而要想完成完整地推理链必须要做多跳路径的决定。路径寻找算法提供了更灵活的多跳路径搜寻，同时，KG中的路径寻找本身就是知识推理的过程，因此可以很好地应用于**问题回答**和**对话系统**当中没有明显答案的场景。这其中的主要挑战在于定位目标实体集合并在回复中提到它们，这些相连的实体往往遵循自然概念链，帮助生成更有逻辑、更富有内容的回复。基于路径的方法探索了节点之间连接的不同模式，并学习了KG中可追溯的路径来为文本生成提供额外指导。PF方法主要分为**路径排序**和**强化学习**两大类：

**①路径排序**

路径排序算法（path ranking algorithm, PRA）使用随机游走，通过多重有界深度优先搜索（DFS）算法寻找关联路径。结合弹性网络（elastic-net）学习，PRA选择了合理的路径来修剪虽然并不理想但是事实正确的KG路径。而在**解码**阶段，PRA从两个网络——门控循环单元（GRU）网络和基于PRA的多跳推理网络中挑选输出。然而，PRA的主要缺点在于操作空间是完全离散化的，很难在KG中找到相似的实体和联系。Bauer等人提出了使用三步策略对路径进行排序和过滤的方法：初始节点评分、累计节点评分和路径筛选 。Ji等人启发性地修剪了实体节点间的噪声边，并提出了沿多跳路径传播边上概率的路径算法。 

**②强化学习**

基于强化学习的推理方法建立在连续空间上，这种方法在路径寻找的奖赏函数中结合了不同的判别方法。Xiong等人提出了DeepPath，应用了Markov决策过程（MDP）和强化学习的方法 。然而，MDP的状态要求事先知道目标实体，所以路径搜寻也依赖于答案实体。因此在大部分QA和对话场景中并不适用。近年来的强化学习推理方法在非生成性的QA场景如**答案检索**和**阅读理解**中的表现十分吸引人。

基于强化学习的路径搜寻方法通常包括两部分：首先获得从 $\mathcal G$ 中获得起始节点 $u_0$ ，进行多跳图谱推理，到达包含相应知识的目标节点 $u_k$ ；第二步是使用两个分离的编码器对输入序列 $X$ 和路径 $\Phi_k(u_0,u_k)$ 进行表示，并通过注意力机制进行解码。基于路径的知识图谱推断将图结构转化成了路径的线性结构，使得它能够被序列编码器表示。强化学习的总体生成损失函数定义为 ${\mathcal L}_{KG-RL-NLL}(\theta)=-\sum^m_{t=1}\log(p(y_t|y_{<t},\Phi_k(u_0,u_k),X))$ 。

同时，Niu等人提到前面的方法不能有效的利用长文本信息，他们使用了一个机器阅读理解模型和一个双线性模型来对全局和局部可能的节点打分 。而Xu等人提出了一个对抗元学习算法来使用动态KG促进对话生成 。

#### 4.2.3 使用**GNN**增强图的表示

新兴的GNN和Graph2Seq技术将图表示学习和文本生成连接了起来。许多NLG任务的重点不在推理过程，而在于对全局上下文的理解。例如摘要总结任务需要结构化的表示来利用相关实体间的联系并保留全局上下文的信息，图表示方法提供了结构化的总结，同时突出了相关概念的关联性。GNN等方法在丰富语义和结构化知识的融合中起到了重要的作用。

**①编码增强**

在编码阶段，结合KG的一个普遍方法是通过结合相应实体节点向量的方法增强单词的语义，使用一个事先定义好的实体连接函数 $\psi:{\mathcal V\times X\rightarrow U}$ 将输入序列中的单词表示成KG中相应的实体节点。对一个输入序列而言，所有连接的实体和他们的K跳近邻都会被表示成序列相关的K跳子图 ${\cal G}_{sub}$ ，该子图同样使用KG的结构及其实体和边的特征来学习每个实体节点的表示向量 $\bf u$ ，再通过 ${\rm R\scriptstyle EADOUT}(\cdot)$ 函数学习子图表示 ${\bf h}_{subG}={\rm R\scriptstyle EADOUT}(\{\bf {u},u\in{\mathcal U}_{sub}\})$ 。

除此之外，近期还有一些研究将输入KG转化成其等价Levi图，Levi图中的实体节点和关系边地位相同。在这一过程中，边由其他两个节点替代：一个表示关系的向前方向，另一个表示相反方向。这些节点和实体节点相连，从而保存了边的方向信息。由此可见，这种方法重新组织了原有知识图谱的架构，变为了无标注图。

**②解码增强**

在解码阶段，起始的隐藏层使用了子图表示进行增强： ${\bf s}_0={\bf h}_n\oplus{\bf h}_{subG}$ 。KG增强的解码器可以通过读取得到的子图来生成结合图信息的上下文向量，并用这个向量更新解码状态，以自适应地从得到的子图中选择一个一般的单词或实体。而鉴于图级别的注意力可能会忽略一些细微的知识，现有的研究一般都会采用层次图注意力机制，这种机制会读取子图 ${\mathcal G}_{sub}$ 和它包含的所有知识边，解码器会使用 ${\bf s}_t=f_{de}({\bf s}_{t-1},{\bf c}_t,{\bf c}_{t-1}^{kg},{\bf c}_{t-1}^{ke},{\bf e}(y_{t-1}))$ 进行隐状态更新，其中 ${\bf c}_{t}^{kg}$ 和 ${\bf c}_{t}^{ke}$ 分别代表 $t$ 步时图级别上下文向量和边级别上下文向量。层次图注意力机制同时关注了子图和细微的实体和关系。另外， ${\bf c}_{t}^{kg}$ 向量是实体节点向量的加权和，衡量了 ${\bf s}_t$ 和 $\bf u$ 之间的联系。

在一些相关研究中，Wang等人提出的PaperRobot可以迭代生成知识图谱、预测概念之间的新联系并输出论文草稿 。Zhang等人提出的ConceptFlow可以对可能的对话流进行表示，这些对话流可以沿着尝试常识关系在概念空间中进行遍历，这种遍历是图注意力机制（GAT）引导的 。具体来说，ConceptFlow能够在会话话语中沿着常识关系，跳跃到较远但是具有意义的概念，引导模型生成更具信息量和更扣题的回复。

#### 4.2.4 不同方法的优缺点

KGE方法将KG中的成分嵌入到连续向量空间中，在保留了KG固有结构的同时也简化了操作，嵌入的实体和关系可以用来增强输入文本表示。然而KGE无法对复杂的关系路径进行模型表示，它常常作为KG结合的一种基础方法。

对路径推断和GNN的选择主要取决于使用知识图谱的目的和应用。关系路径推断使用了图结构中的路径信息，从而可以进行间接推理，问题回答和对话系统便是最重要的应用之一。GNN则通过迭代整合临近节点和边的信息来学习图中的多跳连接结构，提供了与输入文本中实体单词相关的结构化知识，帮助机器理解全局上下文内容。

### 4.3 其他图结构增强的NLG

#### 4.3.1 内部知识图谱（open KG）

上面提到的知识图谱都建立在输入文本以外的信息，也称外部知识图谱（external KG）。与之相反的是，只依赖输入文本的知识图谱则被称作内部知识图谱（internal KG）。

内部KG在理解输入序列，尤其是那些非常长的输入序列中，扮演着重要的角色。通过构建内部KG，模型可以融合或丢弃冗余信息，产生一个精简的形式来表示输入文本。另外，对KG的表示也可以产生一个结构化的概括总结并突出相邻概念的关联性。一种构建内部知识图谱的主流方法是使用开放信息提取（OpenIE），与传统的信息提取（IE）方法不同，OpenIE不会受限于目标实体和关系事先已知的小集合，而是会提取输入文本的所有实体及关系的信息，有助于从文本中提取发现领域无关的关系，并扩展延伸到大型异构语料库。

在获取内部KG后，下一步要学习内部KG的表示并将其整合进生成模型里。Zhu等人使用图注意力网络（GAT）对每个节点进行表示，对将其融合进Transformer架构 。Huang等人延伸了这一模型，他们首先使用GAT把每一段落编码成一个子图，再使用Bi-LSTM方法把子图连接起来 。Fan等人通过多输入文本场景构建了内部KG，这种图谱将多个输入文本显著地压缩，并通过合并操作减小了冗余性，他们同时也在标准Graph2Seq中使用了分层和记忆压缩注意力机制 。

#### 4.3.2 句法依赖图谱

句法依赖图谱是一个用来表示单词之间句法联系的有向无环图，这种增强的句子表示通过利用依赖信息，可以捕捉到长距离依赖约束和关系。在NLG任务中，依赖关系往往以以下三种模型进行表示：①线性表示：线性化依赖图谱并使用序列模型获得融入语义关系的表示 ②路径表示：单词和依赖图谱中中心位置的距离与它对上下文向量的贡献正相关 ③图表示：使用GNN来聚合信息。

#### 4.3.3 语义依赖图谱

语义依赖图谱能对单词间的谓语-论元关系进行表示，并且基于不同的标注系统拥有不同的表示方法。语义依赖图谱中的节点是通过语义角色标注（semantic role labeling, SRL）或者依赖关系分析（dependency parsing）进行提取的，并根据不同的语义内和语义间关系进行连接。由于语义依赖图谱引入了一种更高层次的信息，它能够捕获相同谓词-论元结构的不同实现方法之中的共性，因此被广泛用于文本生成效果的提升。

在近期研究中，Jin等人提出了语义依赖图谱引导的总结归纳模型，他们通过堆叠编码器的方法对依赖图谱进行融入操作，这些编码器包括一个序列编码器和一个图编码器，序列编码器首先通过多头注意力机制读入文本，再由图编码器编码语义关系并将语义图谱结构融入文本级别的表示中 。同时还有一些研究使用了抽象含义表示（abstract meaning representation, AMR）的方法，它能够直接捕获实体关系并抽象出词形变化和虚词，以帮助增强语义的保留并有效地处理数据稀疏情况。

#### 4.3.4 关键词句图谱

与使用序列编码器表示提取关键词的方法不同，近期的一些研究提出了通过构建关键词交互图谱来表示（尤其是较长）文档的方法，它可以反映整篇文本的结构。构建关键词图谱需要两步：首先将文本分解成几个关键词为中心的文本块，每一块形成了图谱中的一个节点；第二步再将文本中的每一个句子和相应的关键词节点进行连接，再基于语义联系构建节点之间的边。注意到一句话可以连接多个关键词节点，而这实际上就隐含着两个关键词之间的联系。

## 5.标注文本增强NLG

知识标注文本指的是能提供额外相关知识的文本信息，这些信息往往无法在语料库或者结构化数据库里找到，但是可以从百科、购物网站等在线资源获得。知识标注文本可以很好地帮助模型理解输入序列及上下文，因此在提升**对话系统**中的互动体验方面得到了广泛的研究。根据信息来源和使用方法的不同，这些研究可以被分为两类。**第一类的方法**遵循两步走过程：首先从原始文本条目（如Wikipedia）或者搜索引擎（Google）中检索段落，得到前k个结果，作为关系最紧密的知识；第二步将对话历史和检索段落加入神经生成模型中。**第二类方法**叫基于背景的对话（background based conversation，BBC）。BBC建立了一种对话模式，可以从给定文本中获取相关信息。

除了对话系统外，在**摘要生成**领域，只基于Seq2Seq的模型效果可能并不好（单词过少或者同一单词大量重复），另外，Seq2Seq模型还经常会把源单词复制到相同的位置。而利用文本摘要作为模板可以为生成过程提供参照。而在**问题回答**领域，如果只靠给定问题很难生成合适的答案，所以也需要更多额外的信息。下表总结了使用知识标注文本增强的几种方法，主要可以分为两类：检索信息指导生成和对背景文本建模指导回复生成。

<div align="center">
    <img src="https://pic2.zhimg.com/v2-a5f40db11d2937551858fa0490cc3619_b.png">  
</div>

### 5.1 检索信息标注文本

由于语料库中不存在知识标注文本，因此这一方法的主要思想是基于输入文本，从外部资源中检索相关文本信息并将其融入进生成过程中。这一过程和知识获取已经KB、KG的融合十分相似，区别在于标注文本往往是非结构化而充满噪声的，所以研究者们着重于研究知识选择和融入方法。

#### 5.1.1 检索相关段落

在对话系统和问题回答过程中，人们往往会根据需要搜索获取外部信息。然而，由于大部分常识在训练语料库中并没有提供，所以构建一个完全数据驱动的对话或问答系统也十分困难，它们可能会避免产生有意义的回复。一个智能的机器应当检索相关事实，并且回顾那些必要的背景信息。检索事实有许多种方法，包括通过命名实体进行匹配；使用统计方法（如TF-ID和BM25）在大型文本集中对相关文本进行评分。Ghazvininejad等人提出了知识标注神经对话模型（KGNCM），模型首次能够从Foursquare和Twitter中获取信息 。接着，模型将检索到的信息融入了对话回复生成中，并使用端到端记忆网络来根据选择的片段 $K$ 进行回复。记忆网络首先在所有检索到的片段上计算注意力表示 ${\bf h}_K$ ，得到对话上下文相关知识的注意力表示如下：

<div align="center">
    <img src="https://pic3.zhimg.com/v2-eff8200190bfb85369da37e223568d02_b.png">  
</div>

其中 ${\bf h}_X\in \mathbb{R} ^d$ 代表输入序列嵌入， ${\bf h}_{k_i}\in\mathbb{R}^d$ 代表第i个检索事实段落的嵌入。然而，该模型并没有增强解码阶段。为了从所有可用的事实中获得单词作为可用知识，Yavuz等人使用了分层指针网络来得到复制单词的概率 ，在解码阶段，该模型不仅关注了知识事实来获取每个片段的重要性，还使用了拷贝机制来从段落中拷贝有用的单词。而在问答场景中，Chen等人 和Gao等人 将网上的顾客评论融入到了回答答案中。

#### 5.1.2 检索并重新排列软性模版

在**摘要生成**领域，只依赖输入文本的Seq2Seq模型生成的摘要会随字数的变多逐渐变差，它们会经常生成无关和重复的词语。而基于模板的摘要生成假设完美总结（golden summaries）能够提供参考点指导输入序列的摘要生成，这些模板常被叫做软性模板（soft templates）以和传统的基于规则的模板区分。基于软性模板的摘要生成过程包含三个阶段：**检索**、**重排**和**重写**。检索阶段通过文本相似度，从一系列摘要集中挑选几个候选模板，重排模块则从这几个候选模板中挑出和真实摘要 $Y$ 最相似的模板 $T$ ，其中真实显著性 $S^*(T,Y)$ 由ROUGE方法计算得到，模板预测显著性 $S(T,X)$ 由双线性网络计算得到，预测显著性应和真实显著性尽可能接近。最后重写阶段根据原始文本和最佳模板生成更有信息量的总结。总体的损失函数为：
$$
\begin{aligned} {\mathcal L}_{ST}(\theta)&={\mathcal L}_{Rerank}(\theta)+{\mathcal L}_{ST-NLL}(\theta)\\&=-[S^*(T,Y)\log S(T,X)+\log(p(y_t|y_{<t},X,{\rm argmax}_{T\in{\mathcal T}}p(T|X)))] \end{aligned}
$$
另外，Wang等人还提出了一个双向选择编码模型（BiSET） ，能够从文章和其模板中相互选择关键信息来辅助摘要生成。

### 5.2 背景建模标注文本

检索到的段落适合输入序列局部相关的，而背景文档则拥有更多的全局知识，可以帮助生成更具信息量的回复，保证对话不偏题。对谈话进行背景文档标注的方法又叫BBC，BBC方法常被用来生成基于一个或多个背景文本的富有信息量的回复。BBC任务常和机器阅读理解（MRC）进行比较，MRC也需要及其从背景中提取信息回答问题。BBC的挑战在于，不仅要定位到正确的语义单元，还要在正确的时间和位置对照正确的背景信息进行回复生成。

鉴于MRC模型往往会把多个语义片段结合在一起来提供答案，许多BBC模型也借鉴了这一思想，通过将不同的信息连接到一起，找到合适的背景知识来生成回复。MRC模型可以高效地将输入序列 $X$ 当作QA中的问题（如SQuAD），和背景文件 $B$ 一起进行编码，MRC常常使用加入了话语信息的背景表示 $\{\bf {h}_i\}^{|B|}_{i=1}$ 作为解码阶段的输入，所以上下文向量 ${\bf c}_t^{rc}$ 不再使用原本输入序列隐藏层的加权和表示，而是使用背景表示的加权和， ${\rm RC}(\cdot,\cdot)$ 为阅读理解模型：

<div align="center">
    <img src="https://pic2.zhimg.com/v2-9720d43e1df79c06cbe7924905f5d941_b.png">  
</div>

尽管背景表示 $\{\bf {h}_i\}^{|B|}_{i=1}$ 在话语和背景之间进行了软连接，但它并没有定位背景的信息，也没有明确地使用小片段来指导文本生成。因此，Meng等人提出了加入参照信息的网络（RefNet）来解决这一问题 ，他们加入了一个新型的参照信息解码器来从背景信息中选取语义单元。而Ren等人认为这种方法没有从整体的角度上看待问题，所以他们引入了全局到局部知识选择机制（GLKS） 。GLKS先学习了一个主题转移向量来对下一回复中最有可能用到的文本片段进行编码，再将其用于指导局部知识的选择。Bi等人将BBC的想法扩展到给予背景信息的QA场景中，他们提出了一个随机选择网络，可以直接根据问题和背景文件之间的关系选择单词来生成回复，单词主要来源于问题、背景文件和预定义词库 。

## 6.未来研究方向及结论

NLG模型的应用还存在着几个开放问题和研究方向。首先，设计更有效的方法来表示知识并将它们整合进生成过程仍然是知识增强文本生成系统中最重要的潮流；第二，如何设计能够学习发现更广阔的知识的方法、组合多种不同来源的不同形式的知识也需要研究人员更多地投入；另外，多任务学习可以提升知识表示和文本生成的表现、结构化知识和非结构化知识可以在增强过程中相互替代。作者总结了三大未来研究的趋势：预训练、元学习和终身学习。

### 6.1 预训练

预训练能够使用大量无标注数据，在许多下游任务处理的表现都十分优秀。而在实际应用中，直接进行预训练的的模型在故事生成任务上依旧会面临知识不足的问题。因此，在预训练模型中引入知识可能是一种解决办法。另一方面，微调作为预训练知识迁移到下游现有研究常使用任务的主要方法之一，实际上从预训练模型中发现知识的途径可以更加多样化，如知识净化（knowledge distillation），数据增强等等。

### 6.2 元学习

目前的NLG研究大多是针对大量的标记数据进行的，然而现实应用中，新领域往往只有少数示例可用。由于收集知识的人力成本较高，外部知识库也往往不完整。因此，快速适应新领域是文本生成任务中的一项重要任务。解决这些问题的一个潜在途径是元学习。元学习在NLG的背景下是指，生成模型在训练时开发了一套广泛的技能和模式识别能力，只需要很少的示例就可以迅速适应一项新任务。因此，建立高效的元学习算法只需对少量的任务进行具体的微调，即可快速学习新任务。

### 6.3 终身学习

终身学习作为一种新兴的机器学习方法，可以不断学习并积累以前的任务中学习到的知识，进而指导未来的学习。在这一过程中，机器学习新知识的效率夜会越来越高。然而，现有的知识增强文本生成系统通常不能实时地更新知识。所以不断更新从各种信息源获取的知识，给机器提供新的知识进而提高文本生成任务的表现，是很有前途的研究方向。

### 6.4 研究结论

综上所述，本研究旨在回答知识增强文本生成中常见的两个问题：**如何获取知识**以及**如何整合知识**。在知识获取方面，本文根据知识获取的三个不同来源进行了介绍。在知识整合方面，本文提出了知识整合的一般方法，并进一步讨论了知识整的一些具体思路和技术解决方案。