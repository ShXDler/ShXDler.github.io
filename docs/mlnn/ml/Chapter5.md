# 5.1 神经元模型

M-P神经元接收输入，整合数值，通过激活函数输出。

# 5.2 感知机与多层网络

如果两类模式线性可分，感知机的学习过程是收敛的，否则会发生震荡。

常用多层前馈神经网络（multi-layer feedforward neural networks）

# 5.3 反向传播算法

反向传播（Backpropagation，BP）——链式求导法则

BP神经网络可能出现过拟合现象，常用两种策略缓解：（1）提前停止（early stopping）：一旦训练集误差降低而验证集误差升高则停止训练。（2）正则化（regularization）：在目标函数中加入模型复杂度惩罚项。

# 5.4 全局最小和局部极小

跳出局部极小的启发式方法：（1）参数取不同初始值（2）使用“模拟退火”（simulated annealing）方法，以一定递减概率跳出局部极小（3）随机梯度下降（4）遗传算法（genetic algorithms）等。

# 5.5 其他常见神经网络

## 5.5.1 RBF网络

RBF（Radial Basis Function，径向基函数）网络是单隐层前馈神经网络，使用径向基函数作为激活函数，网络结构可表示为：

$$\varphi(x)=\sum^q_{i=1}w_i\rho(x,c_i)$$

$q$为隐藏层神经元个数，$c_i$和$w_i$为第$i$个隐层神经元对应的中心和权重，$\rho(x,c_i)$为径向基函数（沿径向对称的函数），通常定义为$x$到$c$之间欧式距离的单调函数，常用高斯径向基函数：

$$\rho(x,c_i)=\exp(-\beta_i||x-c_i||^2)$$

## 5.5.2 ART网络

ART（Adaptive Resonance Theory，自适应谐振理论）网络包括比较层、识别层、识别阈值和重置模块。比较层负责接收输入样本，识别层互相竞争，计算输入向量与每个神经元的代表向量之间的距离，选择最小的产生获胜神经元，如果二者的相似度大于阈值，就将当前输入样本归为代表向量的类别，同时网络权重也会更新；如果相似度小于阈值，重置模块就在识别层新增一个新的神经元，代表向量设置为当前输入向量。

ART较好地缓解了竞争型学习中的“可塑性-稳定性窘境”（stability-plasticity dilemma），既有学习新知识的能力，还可以保持对旧知识的记忆。ART可以进行“增量学习”（incremental learning）或“在线学习”（online learning）。

## 5.5.3 SOM网络

SOM（Self-Organizing Map，自组织映射）网络是也一种竞争学习型的无监督神经网络，将高维数据映射到低维（通常是二维）空间，并保留了原有拓扑结构。

<div align="center"><img src="https://picgo-1305404921.cos.ap-shanghai.myqcloud.com/20210405222617.png" alt="image-20210405222617487" style="zoom:80%;" /></div>

SOM的输出层神经元以矩阵形式排列在二维空间中，每个神经元都有一个权向量，在接受输入后网络会计算每个神经元的权向量与输入向量的距离，选择最短的成为获胜神经元，作为其输出位置。最佳匹配神经元和邻近的神经元的权向量将被调整，缩小与当前样本的距离。

## 5.5.4 级联相关网络

一般的网络模型通常固定好了结构，级联相关（Cascade-Correlation）网络是结构自适应网络的重要代表。它主要有级联和相关两个成分，在开始时只有输入和输出层，随着训练进行不断加入新的隐藏层神经元，形成级联结构。而新的隐藏层神经元加入时，其输入端连接权重是固定的，通过最大化新神经元的输出与网络误差之间的相关性来训练参数。

<div align="center"><img src="https://picgo-1305404921.cos.ap-shanghai.myqcloud.com/20210405223956.png" alt="image-20210405223956683" style="zoom:80%;" /></div>

级联相关网络训练速度较快，但数据量小时容易过拟合。

## 5.5.5 Elman网络

循环神经网络（recurrent network，RNN）允许网络中出现环形结构，Elman网络是代表之一。

## 5.5.6 Boltzmann机

Boltzmann机定义了模型网络的“能量”（energy）。它的结构包含显层和隐层：

<div align="center"><img src="https://picgo-1305404921.cos.ap-shanghai.myqcloud.com/20210405224154.png" alt="image-20210405224154186" style="zoom:80%;" /></div>

玻尔兹曼机中的神经元都是布尔型的，令$s$表示神经元的状态，$w_{ij}$表示神经元$i$和$j$间的连接权重，$\theta_i$表示神经元$i$的阈值，则能量定义为：

$$E(s)=-\sum^{n-1}_{i=1}\sum^n_{j=i+1}w_{ij}s_is_j-\sum^s_{i=1}\theta_is_i$$

$s$出现的概率只由其能量和状态向量空间中所有可能的能量决定：

$$P(s)=\frac{\exp(-E(s))}{\sum_t\exp-E(t)}$$

玻尔兹曼机的训练过程是将每个样本都看作一个状态向量，使其出现的概率尽可能大。标准玻尔兹曼机是一个完全图，复杂度很高，现实生活中常用只保留隐层和显层连接的受限玻尔兹曼机（Restricted Boltzmann Machine，RBM），是一个二部图。

受限玻尔兹曼机常用“对比散度”（Contrastive Divergence，CD）算法进行训练，假设网络有$d$个显层神经元和$q$个隐层神经元，令$v$和$h$分别代表显层和隐层的状态向量，由于同一层内没有连接，有：

$$P(v|h)=\prod_{i=1}^dP(v_i|h)$$
$$P(h|v)=\prod_{j=1}^qP(h_j|v)$$

CD算法对每个训练样本$v$，计算出隐层神经元状态的概率分布，然后根据这个分布采样得到$h$，再从$h$产生$v'$，再从$v'$产生$h'$。连接权重的更新公式为：

$$\Delta w=\eta(vh^\top-v'h'^\top)$$

# 5.6 深度学习

将低层的特征抽象出高层特征，可以将深度学习理解为“特征学习”（feature learning）或“表示学习”（representation learning）。

BP算法可能不能直接用于多隐藏层网络，无监督逐层训练（unsupervised layer-wise training）可以每次训练一层隐藏层，将上一层的输出作为输入，又叫“预训练”（pre-training）。在整体预训练完成后，再进行“微调”（fine-tuning）。例如深度信念网络（deep belief network，DBN）每层都是一个受限玻尔兹曼机，在进行无监督逐层训练时，先训练第一层，再将第一层的输出作为输入训练第二层，以此类推。“预训练”+“微调”可以被看做是将参数先进行分组，先找到局部最优结果，再联合起来进行全局优化。

另一种方法是“权共享”（weight sharing），让一组神经元使用相同的连接权重。例如CNN中的卷积核。

# 5.7 扩展阅读

除了M-P神经元外，还有考虑电位脉冲发放时间的脉冲神经元（spiking neuron）模型。

BP算法实际上是LMS（Least Mean Square）的推广。将LMS推广到前馈网络中就得到了BP算法，因此BP算法也称为广义$\delta$原则。